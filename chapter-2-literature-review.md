# Chapter 2: Literature Review

## 2.1 Introduction

This chapter provides a review of the literature on AI-enhanced software testing, with particular emphasis on fuzzing techniques applicable to automotive C-based software. The review synthesizes research from traditional AI approaches in software testing, large language models for code generation, modern fuzzing methodologies, and relevant automotive safety standards.


## 2.2 Traditional AI Approaches in Software Testing

Genetic algorithms were among the earliest AI applications to software testing. These evolutionary approaches maintain populations of test cases that undergo selection, crossover, and mutation operations over successive generations. Wang et al. \cite{MLFuzzReview2020} documented how evolutionary testing dominated research from the late 1990s through the early 2010s, with test generation formulated as an optimization problem where a fitness function based on coverage or fault detection guides evolution toward effective test inputs. However, evolutionary approaches showed limitations over time, as fitness function design proved challenging and maximizing code coverage did not necessarily correlate with bug detection. Multi-objective optimization helped balance competing goals, but genetic algorithms fundamentally lacked the ability to reason about program semantics and could not understand what made certain inputs more meaningful than others.

Symbolic execution offered an alternative paradigm based on constraint solving rather than random search. The approach treats program inputs as symbolic values and accumulates path constraints during execution, with solving these constraints yielding concrete inputs guaranteed to exercise specific program paths. Godefroid, Levin, and Tauth demonstrated the practical viability of this approach with SAGE \cite{SAGE2008}, which Microsoft deployed internally for security testing, processing billions of machine instructions and finding vulnerabilities that conventional testing had missed. Despite its elegance, symbolic execution faces fundamental scalability challenges, as the number of paths through a program grows exponentially with the number of branch points in a phenomenon known as path explosion. Yavuz's recent work on DESTINA \cite{DESTINA2024} addresses this through targeted execution strategies, but complete path coverage remains infeasible for non-trivial programs, motivating hybrid approaches that combine symbolic reasoning with other techniques.

Machine learning entered software testing research in supporting roles around 2015. A technical report from Sandia National Laboratories \cite{MLFuzzReviewSandia2019} surveyed early applications including defect prediction, test case prioritization, and coverage estimation. These applications used ML models to inform human decision-making rather than to generate tests directly, and while practical and useful, they did not fundamentally change how tests were created. Neural networks enabled more ambitious applications, with Pei et al. \cite{DeepXplore2017} introducing DeepXplore for testing deep learning systems and defining neuron coverage as a testing adequacy criterion. Odena et al. \cite{TensorFuzz2019} extended coverage-guided fuzzing to neural networks with TensorFuzz, demonstrating that coverage-based feedback could guide test generation for ML models themselves. These works showed that neural networks could participate actively in the testing process rather than merely supporting it, marking a transition toward generative approaches that accelerated dramatically with the advent of large language models.


## 2.3 Large Language Models and Code Generation

The foundation for modern large language models was established with the Transformer architecture, which enabled training on vastly larger datasets than previous approaches permitted. Subsequent scaling of Transformer-based models revealed emergent capabilities, including the ability to generate coherent text and syntactically valid source code. GPT-3, released in 2020 with 175 billion parameters, demonstrated that models trained primarily on next-token prediction could perform a wide range of tasks with minimal task-specific training. Code-specific models followed rapidly, with Codex powering GitHub Copilot and achieving 28.8% success on the HumanEval benchmark in initial evaluations, rising to 70.2% with multiple sampling attempts. GPT-4 further improved code generation capabilities with better contextual understanding and reduced occurrence of subtle errors, while context windows expanded from 2K to 128K tokens, enabling inclusion of multiple source files, header definitions, and detailed documentation in prompts.

Open-source alternatives have emerged as important counterparts to proprietary models. Meta's LLaMA demonstrated that competitive performance was achievable with smaller, publicly available models, while code-specific variants including StarCoder and CodeLLaMA targeted programming tasks specifically. These models enable local deployment without transmitting proprietary code to external services, a consideration particularly relevant to automotive applications with intellectual property constraints.

Research has probed the boundaries of LLM code generation capabilities systematically. Deng et al. \cite{TitanFuzz2022} found that while models generated syntactically valid code at high rates, the generated code frequently contained semantic issues, with tests compiling successfully but exercising APIs in unintended ways. Wang et al. \cite{LLMMutationTesting2024} observed similar patterns in mutation testing experiments, noting high success rates on simple cases with degrading performance as complexity increased. C presents particular challenges for LLM-based code generation, as training data skews heavily toward Python, JavaScript, and other high-level languages. C's distinctive features such as pointer arithmetic, manual memory management, preprocessor macros, and undefined behavior are less well represented in training data, and empirical evaluations consistently show worse performance on C compared to Python. This gap has direct implications for automotive applications, where C remains the dominant language for embedded systems.


## 2.4 Fuzzing Techniques

Fuzzing originated with Barton Miller's 1988 experiments at the University of Wisconsin, where students subjected UNIX utilities to streams of random input and found that approximately 25% crashed or hung. This finding established fuzzing as a viable technique for uncovering reliability issues in production software. Early fuzzing was entirely random, requiring no knowledge of target program structure, but this simplicity limited effectiveness since random inputs rarely satisfy input validation checks and prevent exploration of deeper program logic.

Coverage-guided fuzzing represented a fundamental advance in the field. AFL (American Fuzzy Lop), released in 2013, introduced lightweight instrumentation to track which code paths each input explored, with inputs that discovered new coverage retained and mutated further while inputs revealing no new coverage were discarded. This feedback loop dramatically improved efficiency by directing search toward unexplored program behavior. AFL spawned an ecosystem of derivative tools, with AFL++ \cite{AFLPlusPlus} adding performance optimizations and additional mutation strategies, LibFuzzer integrating coverage-guided fuzzing into the LLVM toolchain, and Syzkaller \cite{Syzkaller2020} adapting coverage-guided techniques for kernel testing. Google's OSS-Fuzz project deployed these tools at scale, continuously fuzzing hundreds of open-source projects, and Ding and Le Goues \cite{OSSFuzzBugs2021} analyzed bugs found by OSS-Fuzz, documenting vulnerability classes that other testing methods had missed.

Grammar-based fuzzing addressed the structural validity problem inherent in random mutation. Random byte mutations rarely produce syntactically valid inputs for complex formats, as a randomly mutated JSON file is almost certainly invalid JSON. Grammar-based fuzzers generate inputs according to specified grammars, ensuring structural validity while still exploring variations, and this approach proved particularly effective for protocol parsers and file format handlers. Despite these advances, several challenges persist in fuzzing practice. Writing fuzz harnesses, which connect fuzzers to target libraries, remains manual and labor-intensive, and coverage plateaus occur when random mutation cannot discover paths beyond certain checkpoints. These limitations create opportunities for AI-enhanced approaches that can reason about program structure and generate more targeted inputs.


## 2.5 AI-Enhanced Fuzzing

### 2.5.1 LLM-Based Test Case Generation

Large language models offer a fundamentally different approach to test generation by synthesizing tests from specifications, documentation, or source code rather than mutating existing inputs. Deng et al. introduced TitanFuzz \cite{TitanFuzz2022}, demonstrating that LLMs could generate effective fuzz targets for deep learning library APIs by prompting models with API documentation. TitanFuzz produced test cases that achieved higher coverage than manually written tests for several libraries and discovered previously unknown bugs, establishing that LLM-generated tests could provide practical value.

FuzzGPT \cite{FuzzGPT2023} extended this approach with a focus on edge cases, observing that LLMs possess implicit knowledge of typical API usage patterns from training on vast code repositories. By prompting specifically for atypical or boundary-case inputs, FuzzGPT discovered bugs that conventional mutation-based fuzzers overlooked. Fuzz4All \cite{Fuzz4All2023} pursued a more general approach, targeting multiple programming languages and application domains while using LLMs to generate both test inputs and harness code. Coverage improvements over random generation were substantial, though still below expert-written harnesses. Zhang et al. \cite{LLMDriverGenEffectiveness2023} directly evaluated the effectiveness of LLM-generated fuzz drivers compared to manually written alternatives, finding that results varied significantly across libraries. Some LLM drivers achieved 80 to 90 percent of manual driver coverage, while others reached only 50 percent, with performance correlating with documentation quality and API complexity.

Recent work has explored combining LLM generation with traditional techniques. WhiteFox \cite{WhiteFox2023} uses LLMs for targeted compiler fuzzing, CKGFuzzer \cite{CKGFuzzer2024} augments LLM generation with code knowledge graphs, and HyLLfuzz \cite{HyLLfuzz2024} combines LLM-generated seeds with mutation-based fuzzing. These hybrid approaches attempt to leverage LLM strengths while mitigating their limitations. A consistent finding across this literature is the gap between syntactic validity and semantic quality, as LLMs produce code that compiles at high rates but may not exercise meaningful behavior.


### 2.5.2 Neural Network-Guided Fuzzing

Before LLMs dominated attention, researchers explored using smaller neural networks to guide fuzzing decisions, and these approaches remain relevant due to their lower computational overhead. Wang et al. developed NeuFuzz \cite{NeuFuzz2019}, training neural networks to predict which inputs would discover new coverage by learning from historical fuzzing data and observing which mutations proved productive. NeuFuzz achieved 15 to 30 percent coverage improvements over baseline AFL in experiments.

She, Woo, and Brumley introduced NEUZZ \cite{NEUZZ2018}, using neural networks to approximate program gradients. In smooth functions, gradient information indicates which input bytes most influence behavior, and while programs contain discrete branches and are not inherently smooth, neural approximation proved surprisingly effective, with NEUZZ achieving substantial coverage improvements on several benchmarks. CoCoFuzzing \cite{CoCoFuzzing2021} applied these techniques to neural code models, testing whether models like Codex were themselves vulnerable to adversarial inputs and finding that code models could be fooled by carefully crafted inputs. A limitation of neural-guided approaches is the training data requirement, as each target program requires a dedicated model trained on fuzzing data from that program. Transfer learning offers partial solutions, but models trained on one program do not always generalize to others.


### 2.5.3 Reinforcement Learning Applications

Reinforcement learning formulates fuzzing as a sequential decision problem where at each step the agent selects a mutation strategy, observes the outcome, and adjusts future decisions accordingly. BÃ¶ttinger, Godefroid, and Singh explored deep reinforcement learning for fuzzing \cite{DeepRLFuzz2018}, demonstrating that RL agents could learn mutation strategies superior to static heuristics, with the learned policies proving effective in practice despite not being human-interpretable.

BertRLFuzzer \cite{BertRLFuzzer2023} combined BERT language models with reinforcement learning, using the language model to understand input structure and RL to determine mutation locations. The combination outperformed either approach alone. However, reinforcement learning approaches face practical challenges including reward sparsity, as new coverage may occur only every thousands of trials, and substantial training costs that limit applicability to settings where long training times are acceptable.


## 2.6 C and Safety Standards

Automotive software operates within a regulatory framework that shapes development and testing practices. MISRA C defines coding guidelines intended to eliminate dangerous C constructs from safety-critical software, with guidelines restricting dynamic memory allocation, recursion, and pointer operations to produce code that is more constrained but safer. CERT C Secure Coding Standards focus specifically on security, addressing vulnerability classes including buffer overflows, integer overflows, and format string vulnerabilities.

ISO 26262 specifies functional safety requirements for automotive systems, defining Automotive Safety Integrity Levels (ASIL A through D) with increasingly stringent requirements for higher-risk systems. Testing is central to ISO 26262 compliance, with coverage metrics and verification activities specified according to ASIL level. ISO/SAE 21434 addresses automotive cybersecurity specifically, targeting security throughout the vehicle lifecycle and explicitly mentioning fuzzing as a relevant testing technique, reflecting growing regulatory acceptance of the approach. The AUTOSAR framework standardizes software architecture for automotive applications, with standardized interfaces facilitating automated testing since tests can target well-defined APIs rather than project-specific implementations.


## 2.7 Research Gaps and Thesis Positioning

Analysis of the literature reveals several gaps that this thesis aims to address. Integration with development workflows is underexplored, as academic research typically evaluates techniques in isolation while practical deployment requires integration with CI/CD pipelines, code review processes, and compliance workflows. Klooster et al. \cite{CiCdFuzzing2022} examined fuzzing in CI/CD environments and identified practical challenges including speed, reliability, and determinism that benchmark evaluations do not capture.

C-specific evaluation is limited in the literature, as most LLM fuzzing research targets Python or JavaScript with C receiving comparatively little attention. The distinctive challenges of C, specifically memory safety, undefined behavior, and preprocessor complexity, merit dedicated investigation. Works such as ECG \cite{ECG2024} and GDBFuzz \cite{GDBFuzz2023} address embedded systems but remain exceptions rather than the norm. Additionally, semantic quality lacks established metrics, as existing evaluations focus on compilation success and coverage achievement while whether generated tests exercise meaningful behavior rather than simply reaching code through trivial paths is rarely assessed systematically. Automotive-specific evaluation is also rare, with claims of applicability to safety-critical systems common but actual evaluation on automotive software uncommon. SAFLITE \cite{SAFLITE2024} and research on CAN bus testing \cite{CANBusFuzzAI2021} demonstrate feasibility but do not provide comprehensive evaluation frameworks. This thesis positions itself at the intersection of these gaps, investigating LLM-based fuzz harness generation for C libraries in automotive contexts.


## 2.8 Automotive Software Development

Automotive software development has characteristics that distinguish it from other software domains and influence the applicability of AI-enhanced testing approaches. Development cycles in automotive are substantially longer than in consumer software, with a vehicle platform requiring 3 to 5 years from conception to production and software frozen months before launch. This timeline limits the applicability of techniques that assume rapid iteration. Supply chain complexity introduces additional considerations, as modern vehicles incorporate software from numerous suppliers with independent development processes and OEMs may receive compiled binaries without source code access.

The V-model remains the dominant development lifecycle in automotive, with requirements flowing down through system, architectural, and detailed design phases while verification flows up through unit, integration, and system testing. Fuzzing aligns naturally with unit and integration testing but requires consideration of how results feed into broader verification activities. Software-defined vehicles represent an emerging paradigm shift, with companies like CARIAD bringing more software development in-house to enable more agile practices and over-the-air updates allowing post-production modifications previously impossible.

Cybersecurity requirements have increased significantly for connected vehicles. Regulations including UNECE WP.29 R155 mandate cybersecurity capabilities throughout the vehicle lifecycle, and fuzzing directly supports compliance with these requirements by identifying vulnerabilities before deployment. CI Spark \cite{CISpark2023} and similar industry initiatives indicate growing interest in LLM-assisted testing for CI/CD integration, and the practical deployment of such tools in automotive contexts with their specific constraints and requirements represents an area where this thesis aims to contribute.
